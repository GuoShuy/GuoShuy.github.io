---
layout: default
---
<head>
	<meta charset="utf-8">
	<meta name="description" content="ChatMusician">
	<meta name="keywords" content="GPT-4, open-source, vision-language">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>ChatMusician</title>
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.0/css/bulma.min.css">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma-extensions@6.2.7/dist/css/bulma-extensions.min.css">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
	<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
	<!-- The following needs to be inserted somewhere on the page for the player(s) to work. -->
	<script src="https://cdn.jsdelivr.net/combine/npm/tone@14.7.58,npm/@magenta/music@1.22.1/es6/core.js,npm/focus-visible@5,npm/html-midi-player@1.4.0"></script>
	<script src="https://cdn.jsdelivr.net/npm/bulma-extensions@6.2.7/bulma-quickview/dist/js/bulma-quickview.min.js"></script>
<link type="text/css" rel="stylesheet" charset="UTF-8" href="https://www.gstatic.com/_/translate_http/_/ss/k=translate_http.tr.qhDXWpKopYk.L.W.O/am=wA/d=0/rs=AN8SPfq5gedF4FIOWZgYyMCNZA5tU966ig/m=el_main_css">
</head>


<article class="post">
  <h1 style="text-align: center" class="title is-1 publication-title">MUPT:</h1>
  <h2 style="text-align: center" class="title is-2 publication-title">Symbolic Music Generative Pre-trained Transformer</h2>
  <section class="section">
		<div class="container ">
			<!-- Abstract. -->
			<div class="columns is-centered has-text-centered">
				<div class="column is-four-fifths">
					<h2 style="text-align: center" class="title is-3">Abstract</h2>
					<div class="content has-text-justified">
						<p>SMuPT is a series of pre-trained models for symbolic music generation. It was trained on a large-scale dataset of symbolic music, including millions of monophonic and polyphonic pieces from different genres and styles. The models are trained with the LLama2 architecture, and can be further used for downstream music generation tasks such as melody generation, accompaniment generation, and multi-track music generation.</p>
					</div>
				</div>
			</div>
		</div>
	</section>

  {% include disqus.html %}
</article>
